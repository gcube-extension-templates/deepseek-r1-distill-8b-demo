"""
DeepSeek-R1-Distill-Llama-8B â€” Reasoning Model Demo
=====================================================
DeepSeek R1ì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ Llama 8Bì— ì¦ë¥˜í•œ ëª¨ë¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
<think> íƒœê·¸ë¥¼ íŒŒì‹±í•˜ì—¬ ì¶”ë¡  ê³¼ì •ê³¼ ìµœì¢… ë‹µë³€ì„ ë¶„ë¦¬ ì¶œë ¥í•©ë‹ˆë‹¤.

ì¶”ì²œ ì§ˆë¬¸ ìœ í˜•: ìˆ˜í•™ ë¬¸ì œ, ì½”ë”© ë¬¸ì œ, ë…¼ë¦¬ ì¶”ë¡  ë¬¸ì œ
"""

import re
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# =============================================
# ì„¤ì • (ì›í•˜ëŠ” ëŒ€ë¡œ ìˆ˜ì • ê°€ëŠ¥)
# =============================================

# ëª¨ë¸ ê²½ë¡œ (setup.sh ì‹¤í–‰ í›„ ë¡œì»¬ ê²½ë¡œ ì‚¬ìš©)
# setup.shë¥¼ ì‹¤í–‰í•˜ì§€ ì•Šì€ ê²½ìš° Hugging Face Hubì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ
MODEL_ID = "/workspace/models/deepseek-r1-distill-llama-8b"
FALLBACK_MODEL_ID = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"

# ì¶”ë¡  ì§ˆë¬¸ (ìˆ˜í•™/ì½”ë”©/ë…¼ë¦¬ ë¬¸ì œë¥¼ ì…ë ¥í•˜ë©´ <think> ê³¼ì •ì´ ì˜ ë‚˜íƒ€ë‚©ë‹ˆë‹¤)
USER_PROMPT = "í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì˜ 10ë²ˆì§¸ ìˆ˜ë¥¼ ë‹¨ê³„ë³„ë¡œ êµ¬í•˜ì„¸ìš”."

# ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ (ì¶”ë¡  ê³¼ì • í¬í•¨ìœ¼ë¡œ ë„‰ë„‰íˆ ì„¤ì •)
MAX_NEW_TOKENS = 2048

# Temperature (DeepSeek ê³µì‹ ê¶Œê³ : 0.5 ~ 0.7)
TEMPERATURE = 0.6

# 4-bit ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€
# RTX 40 ì‹œë¦¬ì¦ˆ ì „ ë¼ì¸ì—…(ìµœì†Œ RTX 4060 8GB) í˜¸í™˜ì„ ìœ„í•´ ê¸°ë³¸ê°’ True
# VRAM 16GB ì´ìƒ í™˜ê²½ì—ì„œ Falseë¡œ ë³€ê²½ ì‹œ ë” ë†’ì€ í’ˆì§ˆë¡œ ì‹¤í–‰ ê°€ëŠ¥
USE_4BIT = True

# =============================================


def load_model(model_id: str, use_4bit: bool):
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    import os
    # ë¡œì»¬ ê²½ë¡œì— ëª¨ë¸ì´ ì—†ìœ¼ë©´ Hugging Face Hubì—ì„œ ë‹¤ìš´ë¡œë“œ
    if not os.path.exists(model_id):
        print(f"âš ï¸  ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ({model_id})ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   Hugging Face Hubì—ì„œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤: {FALLBACK_MODEL_ID}")
        model_id = FALLBACK_MODEL_ID
    print(f"ëª¨ë¸ ë¡œë”© ì¤‘... ({model_id})")

    tokenizer = AutoTokenizer.from_pretrained(model_id)

    model_kwargs = {
        "torch_dtype": torch.bfloat16,
        "device_map": "auto",
    }

    if use_4bit:
        model_kwargs["quantization_config"] = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
        print("  â„¹ï¸  4-bit ì–‘ìí™” ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤ (VRAM ì ˆì•½)")

    model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)

    print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\n")
    return model, tokenizer


def parse_response(full_text: str) -> tuple[str, str]:
    """
    ëª¨ë¸ ì¶œë ¥ì—ì„œ <think>...</think> ì¶”ë¡  ê³¼ì •ê³¼ ìµœì¢… ë‹µë³€ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤.

    Returns:
        (think_content, answer_content) íŠœí”Œ
    """
    think_match = re.search(r"<think>(.*?)</think>", full_text, re.DOTALL)
    think_content = think_match.group(1).strip() if think_match else ""

    # </think> ì´í›„ì˜ í…ìŠ¤íŠ¸ë¥¼ ìµœì¢… ë‹µë³€ìœ¼ë¡œ ì²˜ë¦¬
    if "</think>" in full_text:
        answer_content = full_text.split("</think>", 1)[1].strip()
    else:
        answer_content = full_text.strip()

    return think_content, answer_content


def generate_response(
    model, tokenizer, user_prompt: str, max_new_tokens: int, temperature: float
) -> str:
    """
    ëª¨ë¸ì— ì§ˆë¬¸ì„ ì „ë‹¬í•˜ê³  ì „ì²´ ì‘ë‹µ(ì¶”ë¡  ê³¼ì • í¬í•¨)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    Note:
        DeepSeek ê³µì‹ ê¶Œê³ ì‚¬í•­:
        - System prompt ì—†ì´ User promptë§Œ ì‚¬ìš©
        - Temperature: 0.5 ~ 0.7 (ê¸°ë³¸ê°’ 0.6)
    """
    # DeepSeek-R1-Distill ëª¨ë¸ì€ system prompt ì—†ì´ ì‚¬ìš© ê¶Œì¥
    messages = [
        {"role": "user", "content": user_prompt},
    ]

    input_ids = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        return_tensors="pt",
    ).to(model.device)

    with torch.no_grad():
        output_ids = model.generate(
            input_ids,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=temperature,
            top_p=0.95,
            pad_token_id=tokenizer.eos_token_id,
        )

    # ì…ë ¥ í† í° ì œì™¸í•˜ê³  ìƒì„±ëœ ë¶€ë¶„ë§Œ ë””ì½”ë”©
    generated_ids = output_ids[0][input_ids.shape[1]:]
    full_output = tokenizer.decode(generated_ids, skip_special_tokens=True)

    return full_output


def main():
    print("=" * 50)
    print("  ğŸ§  DeepSeek-R1-Distill-Llama-8B â€” Demo")
    print("=" * 50)

    # GPU ìƒíƒœ í™•ì¸
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"ğŸ–¥ï¸  GPU: {gpu_name} ({vram:.1f}GB VRAM)")
    else:
        print("âš ï¸  GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤ (ë§¤ìš° ëŠë¦´ ìˆ˜ ìˆìŒ)")

    print()

    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_model(MODEL_ID, USE_4BIT)

    # ì‘ë‹µ ìƒì„±
    print(f"[ì§ˆë¬¸] {USER_PROMPT}\n")

    full_output = generate_response(
        model, tokenizer, USER_PROMPT, MAX_NEW_TOKENS, TEMPERATURE
    )

    # ì¶”ë¡  ê³¼ì •ê³¼ ìµœì¢… ë‹µë³€ ë¶„ë¦¬
    think_content, answer_content = parse_response(full_output)

    # ì¶”ë¡  ê³¼ì • ì¶œë ¥
    if think_content:
        print("--- ì¶”ë¡  ê³¼ì • (Thinking) ---")
        print(f"<think>\n{think_content}\n</think>")
        print()

    # ìµœì¢… ë‹µë³€ ì¶œë ¥
    print("--- ìµœì¢… ë‹µë³€ ---")
    print(answer_content)
    print("=" * 50)
    print("\nâœ… ì™„ë£Œ! run.pyì˜ USER_PROMPTë¥¼ ìˆ˜ì •í•´ì„œ ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”.")
    print("   ìˆ˜í•™, ì½”ë”©, ë…¼ë¦¬ ë¬¸ì œë¥¼ ì…ë ¥í•˜ë©´ ì¶”ë¡  ê³¼ì •ì´ ë” ì˜ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.")


if __name__ == "__main__":
    main()